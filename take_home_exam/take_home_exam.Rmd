---
title: "Untitled"
author: "Felix Seo"
date: "2023-10-11"
output:
  pdf_document: 
    extra_dependencies: ["amsmath"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}

```{r}
library(mvtnorm)
library(tidyverse)
library(rstan)
library(knitr)
```

```{r}
# loading the data
data_toxic <- read_delim("../data/toxic.csv", delim = ",")
data_returns <- read_delim("../data/returns.csv", delim = ",") %>% 
  mutate(across(contains("Stock"), ~as.numeric(.x)))

```

# Task 5

We can make a contour plot of the unnormalized posterior in the range $\alpha \in [-2.5, 5]$ and $\beta \in [-1, 30]$. 

```{r}
# Task 5 b

mean_vec <- c(0, 10)
cov_mat <- matrix(c(4, 12, 12, 100), nrow = 2)

# x and y is vector of values. 
unnorm_post <- function(alpha, beta, x, y, n){
  prob_pi <- 1 / (1 + exp( - (alpha + beta * x)))
  log_lik <- dbinom(y, n, prob_pi, log = TRUE) # log=TRUE gives the log like.
  log_prior <- dmvnorm(c(alpha, beta), mean_vec, cov_mat, log = TRUE)
  log_post <- sum(log_lik) + log_prior
  log_post 
}

seq_alpha <- seq(-2.5, 5, 0.1)
seq_beta <- seq(-1, 30, 1)


data.frame(
  "alpha" = rep(seq_alpha, each = length(seq_beta)),
  "beta" = rep(seq_beta, times = length(seq_alpha))
) %>% 
  mutate(
    un_post = mapply(function(x, y) unnorm_post(x, y, data_toxic$x, data_toxic$y, 5),
                     alpha, 
                     beta)
  ) %>% # our function not vectorized, not work with mutate.
  ggplot(aes(x = alpha, y = beta, z = un_post)) +
  geom_contour_filled()
```

We will now implement a Metropolis-Hastings (MH) sampler to sample from the posterior distribution. We will use a multivariate normal distribution as proposal distribution such that 
\begin{equation*}
  (\alpha_t, \beta_t) | (\alpha_{t-1}, \beta_{t-1}) \sim N((\alpha_{t-1}, \beta_{t-1}), \Sigma_t), \quad 
  \Sigma_t 
  =
  \begin{pmatrix}
  \sigma^2_\alpha & 0 \\
  0 & \sigma^2_\beta
  \end{pmatrix}.
\end{equation*}
Let $\sigma^2_\alpha = 1$ and $\sigma^2_\beta = 5$. We will sample 10000 draws from the posterior using our MH sampler starting with $(\alpha_0, \beta_0) = (-2.5, 1)$. First we will begin with the traceplots for the parameters.   

```{r}
# Task 5 c

# outputs a vector of sequences of theta as 
# c(theta_0, theta_1, ...) = c(alpha_0, beta_0, alpha_1, beta_1, ...).
MH_alg <- function(alpha_init, beta_init, x, y, n = 5, iter){
  prop_cov <- matrix(c(1, 0, 0, 5), nrow = 2)
  #theta_seq <- c(theta_init)
  #theta <- theta_init     
  alpha <- alpha_init
  beta <- beta_init
  alpha_seq <- c(alpha_init)
  beta_seq <- c(beta_init)
  
  for (i in 1:iter) {
    prop_sample <- rmvnorm(1, c(alpha, beta), prop_cov) %>% as.vector() # vector easy to work with
    prop_theta <- dmvnorm(c(alpha, beta), prop_sample, prop_cov) # q(theta^{t-1} | theta^{star})
    prop_theta_star <- dmvnorm(prop_sample, c(alpha, beta), prop_cov) # q(theta^{star} | theta^{t-1})
    
    post_theta_star <- unnorm_post(prop_sample[1], prop_sample[2] , x, y, n)
    post_theta <- unnorm_post(alpha, beta, x, y, n) # f(theta^{t-1} | x)
    
    post_fraction <- exp(post_theta_star - post_theta)
    prop_fraction <- prop_theta / prop_theta_star
    accept_prob <- min(1, post_fraction * prop_fraction) # multiplication since calc prop above.
    
    unif <- runif(1, 0, 1)
    if (unif <= accept_prob) {
      alpha <- prop_sample[1]
      beta <- prop_sample[2]
      alpha_seq <- c(alpha_seq, prop_sample[1])
      beta_seq <- c(beta_seq, prop_sample[2])
    } else if (unif > accept_prob) {
      alpha_seq <- c(alpha_seq, alpha)
      beta_seq <- c(beta_seq, beta)
    } 
  }

  data.frame("alpha" = alpha_seq, "beta" = beta_seq)
}


```

```{r, fig.cap="dafd fd f d f df"}
# Task 5d traceplots 
set.seed(990108)

samples <- MH_alg(-2.5, 0, data_toxic$x, data_toxic$y, iter = 10000) 
samples2 <- MH_alg(0, 2.5, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 2)

samples %>% 
  mutate(group = 1) %>% 
  bind_rows(samples2) %>%
  mutate(time = rep(seq(0, 10000), times = 2)) %>% 
  pivot_longer(cols = c("alpha", "beta"), names_to = "parameter", values_to = "value") %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x = time, y = value, color = group)) +
  geom_line() +
  facet_wrap(~parameter)
  
  

#samples %>% 
#  {mutate(., time = seq(0, length(.$alpha) - 1, 1))} %>% 
#  ggplot(aes(x = time, y = alpha)) +
#  geom_line()

#samples %>% 
#  {mutate(., time = seq(0, length(.$alpha) - 1, 1))} %>% 
#  ggplot(aes(x = time, y = beta)) +
#  geom_line()

samples %>%
  {mutate(., time = seq(0, length(.$alpha) - 1, 1))} %>%
  pivot_longer(cols = c("alpha", "beta"), names_to = "parameter", values_to = "value") %>% 
  ggplot(aes(x = time, y = value)) +
  geom_line() +
  facet_wrap(~parameter, scales = "fixed")
```

We can see from the plot that $\alpha$ has a smaller spread that $\beta$. Visually it is quite difficult to see if convergence have been attained. We should also try many different starting points for the parameters and plotting them to see if the chains visually coincide. A burn-in phase is desired where we remove draws from the beginning of the chains to not be able to distinguish where each chain has started. Still with this we should use a quantitative statistic to infer whether convergence is reached or not. Since if we make traceplots for 6 different starting values of $\alpha$ and $\beta$ we can see the need for quantative methods. 

```{r}
set.seed(990108)

samples2 <- MH_alg(0, 2.5, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 2)
samples3 <- MH_alg(1, 7, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 3)
samples4 <- MH_alg(2.5, 13, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 4)
samples5 <- MH_alg(4, 20, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 5)
samples6 <- MH_alg(5, 26, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 6)

samples %>% 
  mutate(group = 1) %>% 
  bind_rows(samples2, samples3, samples4, samples5, samples6) %>%
  mutate(time = rep(seq(0, 10000), times = 6)) %>% 
  pivot_longer(cols = c("alpha", "beta"), names_to = "parameter", values_to = "value") %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x = time, y = value, color = group)) +
  geom_line() +
  facet_wrap(~parameter)
```

In Figure we can see that it is quite difficult to really be sure of convergence by visual analysis. Let us now look at the marginal distribution of the parameters. 

```{r}
# Task 5d marginal distribution of the parameters 

samples %>% 
  pivot_longer(cols = everything(), names_to = c("parameter"), values_to = "value") %>% 
  ggplot(aes(x = value, y = after_stat(density))) +
  geom_histogram(bins = 100, color = "black", fill = "white") + 
  facet_wrap(vars(parameter), scales = "free_x")
```

The marginal posterior distributions can be seen in Figure ?? for the parameters. The two parameters does not seem to follow the same distribution. It would not be wrong to say that the marginal posterior distribution for $\alpha$ seems to follow a normal distribution. For the parameter $\beta$ however, it has more of a tail and and is not symmetric. Let us now plot a sample of bivariate draws from our algorithm and plot to the contour plot in part b.  

```{r}
set.seed(990108)

test_alpha <- samples %>% select(alpha) %>% slice_sample(n = 2432) %>% {.$alpha}
test_beta <- samples %>% select(beta) %>% slice_sample(n = 2432) %>% {.$beta}

data.frame(
  "alpha" = rep(seq_alpha, each = length(seq_beta)),
  "beta" = rep(seq_beta, times = length(seq_alpha))
) %>% 
  mutate(
    un_post = mapply(function(x, y) unnorm_post(x, y, data_toxic$x, data_toxic$y, 5),
                     alpha, 
                     beta)
  ) %>% # our function not vectorized, not work with mutate.
  ggplot(aes(x = alpha, y = beta, z = un_post)) +
  geom_contour_filled() +
  geom_point(data = NULL, aes(x = test_alpha, y = test_beta))
```


```{r, eval = FALSE}
# testing area partly

#######################
MH_alg <- function(theta_init, iter, x, y, n = 5){
  prop_cov <- matrix(c(1, 0, 0, 5), nrow = 2)
  theta_seq <- c(theta_init) # prop_mean_seq
  theta <- theta_init       # prop_mean
  
  for (i in 1:iter) {
    prop_sample <- rmvnorm(1, theta, prop_cov) %>% as.vector() # vector easy to work with
    prop_theta <- dmvnorm(theta, prop_sample, prop_cov) # q(theta^{t-1} | theta^{star})
    prop_theta_star <- dmvnorm(prop_sample, theta, prop_cov) # q(theta^{star} | theta^{t-1})
    
    post_theta_star <- unnorm_post(prop_sample[1], prop_sample[2], x, y, n)
    post_theta <- unnorm_post(theta[1], theta[2], x, y, n)
    
    post_fraction <- exp(post_theta_star - post_theta)
    prop_fraction <- prop_theta / prop_theta_star
    
    if (is.na(prop_fraction) == TRUE) {
      stop("prop_fraction is wrong")
    } else if (is.na(post_fraction) == TRUE) {
      stop("post_fraction is wrong")
    }
    
    accept_prob <- min(1, post_fraction * prop_fraction) # multiplication since calc prop above.
    
    unif <- runif(1, 0, 1)
    if (unif <= accept_prob) {
      theta <- prop_sample
      theta_seq <- c(theta_seq, prop_sample)
    } else if (unif > accept_prob) {
      theta_seq <- c(theta_seq, theta)
    } 
  }
  
  theta_seq
}

MH_alg(c(-2.5, 0), 3000, data_toxic$x, data_toxic$y)



#####################

test_alpha <- samples %>% select(alpha) %>% slice_sample(n = 2432) %>% {.$alpha}
test_beta <- samples %>% select(beta) %>% slice_sample(n = 2432) %>% {.$beta}

data.frame(
  "alpha" = rep(seq_alpha, each = length(seq_beta)),
  "beta" = rep(seq_beta, times = length(seq_alpha))
) %>% 
  mutate(
    un_post = mapply(function(x, y) unnorm_post(x, y, data_toxic$x, data_toxic$y, 5),
                     alpha, 
                     beta)
  ) %>% # our function not vectorized, not work with mutate.
  ggplot(aes(x = alpha, y = beta, z = un_post)) +
  geom_contour_filled() +
  geom_point(data = NULL, aes(x = test_alpha, y = test_beta))
```

# Task 6

In this part we are given a data set which consists of monthly returns of 6 stocks from two sectors: Information Technology (IT) and Energy. The returns $\textbf{Y}$ are assumed to follow a multivariate normal distribution 
\begin{equation*}
  \textbf{Y}| \boldsymbol{\mu}, \boldsymbol{\Sigma} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})
\end{equation*}
where
$$
\boldsymbol{\mu}|\mu_0, \sigma^2 \sim N(\mu_0 \boldsymbol{1}, \sigma^2 \boldsymbol{I}),
$$
with $\mu_0 \sim N(0, 1)$ and $\sigma^2 \sim$Inv-Gamma(1, 1). The covariance matrix $\boldsymbol{\Sigma}$ is modeled as 
$$
\boldsymbol{\Sigma}|\alpha, \beta \sim \text{Inv-Wishart}(\nu_0 \textbf{P}_0, \nu_0),
$$
where $\nu_0 = 100$ and 
$$
\textbf{P}_0 = 
\begin{pmatrix}
\alpha \textbf{P}_A & \boldsymbol{0} \\
\boldsymbol{0} & \beta \textbf{P}_B  
\end{pmatrix}
, \quad
\textbf{P}_A
= 
\textbf{P}_B
=
\begin{pmatrix}
1 & 0.5 & 0.5 \\
0.5 & 1 & 0.5 \\
0.5 & 0.5 & 1
\end{pmatrix} 
\\
\alpha \sim N(0, 1), \quad \beta \sim N(0,1).
$$
First we will provide some summary statistics for the returns of each stock. 

```{r}
# Task 6 a

data_returns_2 <- data_returns %>% 
  select(-Date) %>% 
  pivot_longer(everything(), names_to = "stock", values_to = "returns") 

data_returns_2 %>% 
  group_by(stock) %>% 
  summarise(
    mean = mean(returns), 
    variance = var(returns), 
    median = median(returns)
  ) %>% 
  kable(align = "c")
```

We can see in Table ?? that the mean is quite small but similar for all stocks. Stock 1 and 3 in the IT sector have negative mean bu so have stock 3 in the Energy sector. The variance is quite similar between the stocks in the same sector but with higher variance for IT stocks than Energy stocks. The median are all positive except for IT stock 1 and 3. Let us also create boxplots for the different stocks.

```{r}
data_returns_2 %>% 
  ggplot(aes(x = stock, y = returns)) +
  geom_boxplot()
```

In Figure ?? we can see the corresponding boxplots for the stocks. We note that there are some extreme cases. Most notably Energy stock 3 and IT stock 1 have some very big negative returns. Only IT stock 3 have an extreme case where the returns are positive up to about 0.25. Now to represent the possible distributions of the stocks we will create histograms. 

```{r}
# histograms for the stocks  
data_returns_2 %>% 
  ggplot(aes(x = returns, y = after_stat(density))) +
  geom_histogram(bins = 100) + 
  facet_wrap(~ stock)
```

From Figure ?? we can make some assessment on the validity of our model setup. All stocks seem to follow a unimodal distribution with mean close to 0. Approximating the returns with a multivariate distribution seems justified. Now we turn to the model setup in \texttt{Rstan}. One thing to note is that we need to restrict the output of the parameters $\alpha$ and $\beta$ to be non-negative. This is since we in the inverse-Wishart distribution need a positive definite matrix as the scale parameter (the $\nu_0 \textbf{P}_0$ in the above). Other than that there is nothing we need to keep in mind and can just define the model in \texttt{Rstan} following the model setup before.

We can take a closer look at the way we define the covariance matrix $\boldsymbol{\Sigma}$. The way we define $\textbf{P}_0$ makes us inclined to think that we want no correlation between the different sectors. However, have in mind that we do not know the properties of the output for the inverse-Wishart distribution with our parameterization. But looking at a table of distributions (see \textbf{BAYESIAN METHODS}) for $V \sim$ Inv-Wishart$(\boldsymbol{\Omega}, \nu)$ where $\boldsymbol{\Omega}$ is a $k \times k$ symmetric and positive definite matrix, we have that the expected value of matrix element $V_{ij}$ is 
$$
\E(V_{ij}) = \frac{1}{\nu - k - 1} \Omega_{ij}^{-1}.
$$
We will not derive the inverse of $\boldsymbol{\Omega}$ but we can say something about the expected value since in our case we have $\boldsymbol{\Omega} = \nu_0 \textbf{P}_0$. Our scale matrix is a diagonal block matrix which means that its inverse is the inverse of each diagonal block. Since each diagonal block, $\alpha\textbf{P}_A$ and $\beta\textbf{P}_B$, is positive definite the inverse exists for each diagonal block and will also be positive definite. So with this we know that the matrix $\boldsymbol{\Omega}^{-1}$ will have zeroes on the upper right and lower left submatrix, namely 
$$
\textbf{P}_0^{-1} = 
\begin{pmatrix}
(\alpha \textbf{P}_A)^{-1} & \boldsymbol{0} \\
\boldsymbol{0} & (\beta \textbf{P}_B)^-{1}  
\end{pmatrix}.
$$
Thus the covariance elements in $\boldsymbol{\Sigma}$ will have expected value equal to 0. So in some way we get uncorrelated sectors, but we do not know the variances of the elements in $\boldsymbol{\Sigma}$ but hopefully it is quite small. If the variance is not small then we may get large correlations between sectors which may impact our results.  

Now let us take a look on the mean vector in our model namely 
$$
\boldsymbol{\mu}|\mu_0, \sigma^2 \sim N(\mu_0 \boldsymbol{1}, \sigma^2 \boldsymbol{I}).
$$
We can see that each element in the vector has mean $\mu_0 \sim N(0,1)$ and variance $\sigma^2 \sim \text{Inv-Gamma}(1, 1)$. Each element in the mean vector is also uncorrelated so that we assume that the expected value of the returns of each stock is independent of the other stocks. Assuming this may be valid because the reasonable part is that we in the returns $\textbf{Y}$ have some correlation between stocks in the same sectors. 

With our model defined and set up in \texttt{Rstan} we can start looking at how well it performs. First we will conduct posterior predictive checks. We will get a sample from the posterior predictive distribution and derive the summary statistics for it. 

```{r}
# Task 6 b

P_alpha <- matrix(c(1, 0.5, 0.5, 0, 0, 0, 
                    0.5, 1, 0.5, 0, 0, 0, 
                    0.5, 0.5, 1, 0, 0, 0,
                    rep(0, 18)
                  ),
                  nrow = 6) 

P_beta <- matrix(c(0, 0, 0, 0, 0, 0, 
                   0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0,
                   0, 0, 0, 1, 0.5, 0.5, 
                   0, 0, 0, 0.5, 1, 0.5,
                   0, 0, 0, 0.5, 0.5, 1
                  ),
                  nrow = 6)

data_stock <- data_returns %>% select(!Date) %>% as.matrix()

model_data <- list(
  "N" = length(data_returns$Date),
  "K" = 6,
  "Y" = data_stock,
  "vec_1" = rep(1, 6),
  "identity_mat" = diag(1, 6), 
  "P_alpha" = P_alpha * 100, 
  "P_beta" = P_beta * 100
  )

model_fit <- stan(
  "stan_model_6.stan",
  data = model_data,
  chains = 4, 
  iter = 20000, 
  warmup = 10000,
  cores = 4
  )

#post_pred_samp <- sampling(stan_model("stan_model_6.stan"), data = list(
#  "N" = length(data_returns$Date),
#  "K" = 6,
#  "Y" = data_stock,
#  "vec_1" = rep(1, 6),
#  "identity_mat" = diag(1, 6), 
#  "P_alpha" = P_alpha * 100, 
#  "P_beta" = P_beta * 100
#  ),
#  chains = 1,
#  iter = 20000)

rstan::extract(model_fit, pars = "y_rep") %>% 
  as.data.frame() %>% 
  rename(
    "Energy.Stock1" = "y_rep.1",
    "Energy.Stock2" = "y_rep.2",
    "Energy.Stock3" = "y_rep.3",
    "IT.Stock1" = "y_rep.4",
    "IT.Stock2" = "y_rep.5",
    "IT.Stock3" = "y_rep.6"
  ) %>% 
  pivot_longer(everything(), names_to = "stock", values_to = "returns") %>% 
  group_by(stock) %>% 
  summarise(mean = mean(returns), variance = var(returns), median = median(returns))


#test <- posterior::extract_variable(post_pred_samp, variable = c("y_rep[1]"))
#rstan::extract(post_pred_samp, pars = "y_rep") %>% 
#  as.data.frame() %>%
#  rename(
#    "y_1" = "y_rep.1",
#    "y_2" = "y_rep.2",
#    "y_3" = "y_rep.3",
#    "y_4" = "y_rep.4",
#    "y_5" = "y_rep.5",
#    "y_6" = "y_rep.6"
#  ) %>% 
#  pivot_longer(everything(), names_to = "stock", values_to = "returns") %>% 
#  ggplot(aes(x = returns, y = after_stat(density))) +
#  geom_histogram(bins = 50, color = "black", fill = "white") +
#  facet_wrap(vars(stock))
           
```

As we can see the summary statistics in Table ?? differ from the ones we got in Table ??. We have larger mean, variances and median for all of the stocks. Some stocks have reversed sign of the means. So our model does not seem to be able to be able tp predict the returns of the stocks that well. This could be a consequence of our model specification where we may have not been able to capture the properties of the data. Let us now look at the convergence diagnostics of our model. 

```{r, eval = FALSE}
#rstan::extract(model_fit, pars = c("mu", "cov_mat")) %>% 
#  as.data.frame() %>% 
  

traceplot(model_fit, pars = "mu")



summary(model_fit)$summary[,"Rhat"] %>% 
  as.data.frame() %>% 
  rownames_to_column("parameter") %>% 
  rename("Rhat" = ".") %>% 
  filter(grepl("mu", parameter) & parameter != "mu_0") 
```

In Figure ?? we can see the traceplots of the mean vector. Visually it seems that we have reached some sort of convergence but we will combine this with The Gelman-Rubin statistic. In \texttt{Rstan} this statistic is calculated and denoted Rhat.   

```{r}
summary(model_fit)$summary[,"Rhat"] %>% 
  as.data.frame() %>% 
  rownames_to_column("parameter") %>% 
  rename("Rhat" = ".") %>% 
  filter(grepl("mu", parameter) & parameter != "mu_0") %>% 
  kable(align = "c")
```

In Table ?? we can see the Rhat values of the mean vector and all of them are less than 1.01 which then imply that convergence of the chains is reached. We will not look at all the traceplots for the parameters since it will become quite many of them. The Rhat value is easy to check for every parameter and when doing this one find that Rhat is less than 1.01 for every parameter in our model. So we seem to get convergence. we can also look at the autocorrelation plots 

```{r}
stan_ac(model_fit, pars = "mu")
```



```{r, eval=FALSE}
# testing area
set.seed(990108)

P_0 <- rnorm(1) * P_alpha + rnorm(1) * P_beta   

test <- LaplacesDemon::rinvwishart(100, 100 * P_0)  

rmvnorm(
  10000, 
  mean = c(-0.07, -0.06, 0.03, -0.02, -0.05, 0.03), 
  test
) %>% 
  as.data.frame() %>% 
  mutate(mean1 = mean(V1), var1 = var(V1))
  
```


