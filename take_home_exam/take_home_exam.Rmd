---
title: "Take Home Exam in Bayesian Methods"
author: "Felix Seo"
date: "2023-10-11"
bibliography: references.bibtex
output:
  pdf_document: 
    extra_dependencies: 
      amsmath: 
      float:
      caption: ["labelfont = bf", "font = small", "font = it"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  fig.height = 3,
  fig.width = 7, 
  fig.pos = "H",
  out.extra = "",
  message = FALSE, 
  warning = FALSE
)
```

\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}

```{r}
library(mvtnorm)
library(tidyverse)
library(rstan)
library(knitr)
```

```{r, out.width="60%", fig.align="center", fig.cap="dfg fdaf"}
include_graphics("IMG_1171.jpg")
```


```{r}
# loading the data
data_toxic <- read_delim("../data/toxic.csv", delim = ",")
data_returns <- read_delim("../data/returns.csv", delim = ",") %>% 
  mutate(across(contains("Stock"), ~as.numeric(.x)))

```

# Task 5

We can make a contour plot of the unnormalized posterior in the range $\alpha \in [-2.5, 5]$ and $\beta \in [-1, 30]$ seen in Figure 2.  

```{r, fig.cap = "Contour plot of the unnormalized log-posterior for $\\alpha \\in [-2.5, 5]$ and $\\beta \\in [-1, 30]$."}
# Task 5 b

mean_vec <- c(0, 10)
cov_mat <- matrix(c(4, 12, 12, 100), nrow = 2)

# x and y is vector of values. 
unnorm_post <- function(alpha, beta, x, y, n){
  prob_pi <- 1 / (1 + exp( - (alpha + beta * x)))
  log_lik <- dbinom(y, n, prob_pi, log = TRUE) # log=TRUE gives the log like.
  log_prior <- dmvnorm(c(alpha, beta), mean_vec, cov_mat, log = TRUE)
  log_post <- sum(log_lik) + log_prior
  log_post 
}

seq_alpha <- seq(-2.5, 5, 0.1)
seq_beta <- seq(-1, 30, 1)


data.frame(
  "alpha" = rep(seq_alpha, each = length(seq_beta)),
  "beta" = rep(seq_beta, times = length(seq_alpha))
) %>% 
  mutate(
    un_post = mapply(function(x, y) unnorm_post(x, y, data_toxic$x, data_toxic$y, 5),
                     alpha, 
                     beta)
  ) %>% # our function not vectorized, not work with mutate.
  ggplot(aes(x = alpha, y = beta, z = un_post)) +
  geom_contour_filled() +
  xlab(latex2exp::TeX(r'($\alpha$)')) +
  ylab(latex2exp::TeX(r'($beta$)')) +
  theme(legend.key.height= unit(0.45, 'cm'), axis.title.y = element_text(angle = 0, vjust = 0.5)) 
```

We will now implement a Metropolis-Hastings (MH) sampler to sample from the posterior distribution. We will use a multivariate normal distribution as proposal distribution such that 
\begin{equation*}
  (\alpha_t, \beta_t) | (\alpha_{t-1}, \beta_{t-1}) \sim N((\alpha_{t-1}, \beta_{t-1}), \Sigma_t), \quad 
  \Sigma_t 
  =
  \begin{pmatrix}
  \sigma^2_\alpha & 0 \\
  0 & \sigma^2_\beta
  \end{pmatrix}.
\end{equation*}
Let $\sigma^2_\alpha = 1$ and $\sigma^2_\beta = 5$. We will sample 10000 draws from the posterior using our MH sampler starting with $(\alpha_0, \beta_0) = (-2.5, 0)$. First we will begin with the traceplots for the parameters.   

```{r}
# Task 5 c

# outputs a vector of sequences of theta as 
# c(theta_0, theta_1, ...) = c(alpha_0, beta_0, alpha_1, beta_1, ...).
MH_alg <- function(alpha_init, beta_init, x, y, n = 5, iter){
  prop_cov <- matrix(c(1, 0, 0, 5), nrow = 2)
  #theta_seq <- c(theta_init)
  #theta <- theta_init     
  alpha <- alpha_init
  beta <- beta_init
  alpha_seq <- c(alpha_init)
  beta_seq <- c(beta_init)
  
  for (i in 1:iter) {
    prop_sample <- rmvnorm(1, c(alpha, beta), prop_cov) %>% as.vector() # vector easy to work with
    prop_theta <- dmvnorm(c(alpha, beta), prop_sample, prop_cov) # q(theta^{t-1} | theta^{star})
    prop_theta_star <- dmvnorm(prop_sample, c(alpha, beta), prop_cov) # q(theta^{star} | theta^{t-1})
    
    post_theta_star <- unnorm_post(prop_sample[1], prop_sample[2] , x, y, n)
    post_theta <- unnorm_post(alpha, beta, x, y, n) # f(theta^{t-1} | x)
    
    post_fraction <- exp(post_theta_star - post_theta)
    prop_fraction <- prop_theta / prop_theta_star
    accept_prob <- min(1, post_fraction * prop_fraction) # multiplication since calc prop above.
    
    unif <- runif(1, 0, 1)
    if (unif <= accept_prob) {
      alpha <- prop_sample[1]
      beta <- prop_sample[2]
      alpha_seq <- c(alpha_seq, prop_sample[1])
      beta_seq <- c(beta_seq, prop_sample[2])
    } else if (unif > accept_prob) {
      alpha_seq <- c(alpha_seq, alpha)
      beta_seq <- c(beta_seq, beta)
    } 
  }

  data.frame("alpha" = alpha_seq, "beta" = beta_seq)
}
```

```{r, fig.cap="Traceplots of $\\alpha$ and $\\beta$ with 10000 iterations."}
# Task 5d traceplots 
set.seed(990108)

samples <- MH_alg(-2.5, 0, data_toxic$x, data_toxic$y, iter = 10000) 


samples %>%
  {mutate(., time = seq(0, length(.$alpha) - 1, 1))} %>%
  pivot_longer(cols = c("alpha", "beta"), names_to = "parameter", values_to = "value") %>% 
  ggplot(aes(x = time, y = value)) +
  geom_line() +
  facet_wrap(~parameter, scales = "fixed")

```

We can see in Figure 3 the traceplots for the parameters and that $\alpha$ has a smaller spread that $\beta$. Visually it is quite difficult to see if convergence have been attained. We should also try many different starting points for the parameters and plotting them to see if the chains visually coincide. A burn-in phase is desired where we remove draws from the beginning of the chains to not be able to distinguish where each chain has started and the chains get well mixed. Still with this we should use a quantitative statistic to infer whether convergence is reached or not. Since if we make traceplots for 6 different starting values of $\alpha$ and $\beta$ we can see the need for quantitative methods. We call each set of starting values and its traceplot a chain. 

```{r, fig.cap = "Traceplots of $\\alpha$ and $\\beta$ with 10000 iterations. Here 6 different starting values of $\\alpha$ and $\\beta$ are considered so we end up with 6 chains for each parameter."}
set.seed(990108)

samples2 <- MH_alg(0, 2.5, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 2)
samples3 <- MH_alg(1, 7, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 3)
samples4 <- MH_alg(2.5, 13, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 4)
samples5 <- MH_alg(4, 20, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 5)
samples6 <- MH_alg(5, 26, data_toxic$x, data_toxic$y, iter = 10000) %>% 
  mutate(group = 6)

samples %>% 
  mutate(group = 1) %>% 
  bind_rows(samples2, samples3, samples4, samples5, samples6) %>%
  mutate(time = rep(seq(0, 10000), times = 6)) %>% 
  pivot_longer(cols = c("alpha", "beta"), names_to = "parameter", values_to = "value") %>% 
  mutate(group = as.factor(group)) %>% 
  ggplot(aes(x = time, y = value, color = group)) +
  geom_line() +
  facet_wrap(~parameter) +
  labs(color = "Chain")
```

In Figure 4 we can see that it seems that we may have reached convergence by visual analysis. But we should also combine this with quantitative methods to be more sure. Let us now look at the marginal distribution of the parameters. 

```{r, fig.cap = "Histograms of the marginal posterior distribution of $\\alpha$ and $\\beta$."}
# Task 5d marginal distribution of the parameters 

samples %>% 
  pivot_longer(cols = everything(), names_to = c("parameter"), values_to = "value") %>% 
  ggplot(aes(x = value, y = after_stat(density))) +
  geom_histogram(bins = 80, color = "black", fill = "white") + 
  facet_wrap(vars(parameter), scales = "free_x")
```

The marginal posterior distributions can be seen in Figure 5 for the parameters. The two parameters does not seem to follow the same distribution. It would not be completely wrong to say that the marginal posterior distribution for $\alpha$ seems to follow a normal distribution. For the parameter $\beta$ however, it has more of a tail and does not seem to be symmetric. Let us now plot a sample of bivariate draws from our algorithm and plot to the contour plot seen in Figure 2. In Figure 6 we can see that the sample seems to be evenly spread around $\beta = 10$ and $\alpha = 1$.  

```{r, fig.cap= "Contour plot of the unnormalized log-posterior for $\\alpha \\in [-2.5, 5]$ and $\\beta \\in [-1, 30]$. Here a sample is plotted onto the contour plot."}
set.seed(990108)

test_alpha <- samples3 %>% select(alpha) %>% slice_sample(n = 2432) %>% {.$alpha}
test_beta <- samples3 %>% select(beta) %>% slice_sample(n = 2432) %>% {.$beta}

data.frame(
  "alpha" = rep(seq_alpha, each = length(seq_beta)),
  "beta" = rep(seq_beta, times = length(seq_alpha))
) %>% 
  mutate(
    un_post = mapply(function(x, y) unnorm_post(x, y, data_toxic$x, data_toxic$y, 5),
                     alpha, 
                     beta)
  ) %>% # our function not vectorized, not work with mutate.
  ggplot(aes(x = alpha, y = beta, z = un_post)) +
  geom_contour_filled() +
  geom_point(data = NULL, aes(x = test_alpha, y = test_beta), size = 0.5) +
  xlab(latex2exp::TeX(r'($\alpha$)')) +
  ylab(latex2exp::TeX(r'($beta$)')) +
  theme(legend.key.height= unit(0.45, 'cm'), axis.title.y = element_text(angle = 0, vjust = 0.45)) 
```


```{r, eval = FALSE}
# testing area partly

#######################
MH_alg <- function(theta_init, iter, x, y, n = 5){
  prop_cov <- matrix(c(1, 0, 0, 5), nrow = 2)
  theta_seq <- c(theta_init) # prop_mean_seq
  theta <- theta_init       # prop_mean
  
  for (i in 1:iter) {
    prop_sample <- rmvnorm(1, theta, prop_cov) %>% as.vector() # vector easy to work with
    prop_theta <- dmvnorm(theta, prop_sample, prop_cov) # q(theta^{t-1} | theta^{star})
    prop_theta_star <- dmvnorm(prop_sample, theta, prop_cov) # q(theta^{star} | theta^{t-1})
    
    post_theta_star <- unnorm_post(prop_sample[1], prop_sample[2], x, y, n)
    post_theta <- unnorm_post(theta[1], theta[2], x, y, n)
    
    post_fraction <- exp(post_theta_star - post_theta)
    prop_fraction <- prop_theta / prop_theta_star
    
    if (is.na(prop_fraction) == TRUE) {
      stop("prop_fraction is wrong")
    } else if (is.na(post_fraction) == TRUE) {
      stop("post_fraction is wrong")
    }
    
    accept_prob <- min(1, post_fraction * prop_fraction) # multiplication since calc prop above.
    
    unif <- runif(1, 0, 1)
    if (unif <= accept_prob) {
      theta <- prop_sample
      theta_seq <- c(theta_seq, prop_sample)
    } else if (unif > accept_prob) {
      theta_seq <- c(theta_seq, theta)
    } 
  }
  
  theta_seq
}

MH_alg(c(-2.5, 0), 3000, data_toxic$x, data_toxic$y)



#####################

test_alpha <- samples %>% select(alpha) %>% slice_sample(n = 2432) %>% {.$alpha}
test_beta <- samples %>% select(beta) %>% slice_sample(n = 2432) %>% {.$beta}

data.frame(
  "alpha" = rep(seq_alpha, each = length(seq_beta)),
  "beta" = rep(seq_beta, times = length(seq_alpha))
) %>% 
  mutate(
    un_post = mapply(function(x, y) unnorm_post(x, y, data_toxic$x, data_toxic$y, 5),
                     alpha, 
                     beta)
  ) %>% # our function not vectorized, not work with mutate.
  ggplot(aes(x = alpha, y = beta, z = un_post)) +
  geom_contour_filled() +
  geom_point(data = NULL, aes(x = test_alpha, y = test_beta))
```

# Task 6

In this part we are given a data set which consists of monthly returns of 6 stocks from two sectors: Information Technology (IT) and Energy. The returns $\textbf{Y}$ are assumed to follow a multivariate normal distribution 
\begin{equation*}
  \textbf{Y}| \boldsymbol{\mu}, \boldsymbol{\Sigma} \sim N(\boldsymbol{\mu}, \boldsymbol{\Sigma})
\end{equation*}
where
$$
\boldsymbol{\mu}|\mu_0, \sigma^2 \sim N(\mu_0 \boldsymbol{1}, \sigma^2 \boldsymbol{I}),
$$
with $\mu_0 \sim N(0, 1)$ and $\sigma^2 \sim$Inv-Gamma(1, 1). The covariance matrix $\boldsymbol{\Sigma}$ is modeled as 
$$
\boldsymbol{\Sigma}|\alpha, \beta \sim \text{Inv-Wishart}(\nu_0 \textbf{P}_0, \nu_0),
$$
where $\nu_0 = 100$ and 
$$
\textbf{P}_0 = 
\begin{pmatrix}
\alpha \textbf{P}_A & \boldsymbol{0} \\
\boldsymbol{0} & \beta \textbf{P}_B  
\end{pmatrix}
, \quad
\textbf{P}_A
= 
\textbf{P}_B
=
\begin{pmatrix}
1 & 0.5 & 0.5 \\
0.5 & 1 & 0.5 \\
0.5 & 0.5 & 1
\end{pmatrix}, 
\quad
\alpha \sim N(0, 1), \quad \beta \sim N(0,1).
$$
First we will provide some summary statistics for the returns in our data set of each stock. 

```{r}
# Task 6 a

data_returns_2 <- data_returns %>% 
  select(-Date) %>% 
  pivot_longer(everything(), names_to = "stock", values_to = "returns") 

data_returns_2 %>% 
  group_by(stock) %>% 
  summarise(
    mean = mean(returns), 
    variance = var(returns), 
    median = median(returns)
  ) %>% 
  kable(
    align = "c", 
    caption = "Showing the mean, variance and median of the returns of the 6 different stocks.")
```

We can see in Table 1 that the mean is quite small but similar for all stocks. Stock 1 and 3 in the IT sector have negative mean but so have stock 3 in the Energy sector. The variance is quite similar between the stocks in the same sector but with higher variance for IT stocks than Energy stocks. The median are all positive except for IT stock 1 and 3. Let us also create boxplots for the different stocks.

```{r, fig.cap = "Boxplot of the returns of the 6 different stocks."}
data_returns_2 %>% 
  ggplot(aes(x = stock, y = returns)) +
  geom_boxplot()
```

In Figure 7 we can see the corresponding boxplots for the stocks. We note that there are some extreme cases. Most notably Energy stock 3 and IT stock 1 have some very big negative returns. Only IT stock 3 have an extreme case where the returns are positive up to about 0.25. Now to represent the possible distributions of the returns of the stocks we will create histograms. 

```{r, fig.cap="Histogram for the returns of the 6 different stocks."}
# histograms for the stocks  
data_returns_2 %>% 
  ggplot(aes(x = returns, y = after_stat(density))) +
  geom_histogram(bins = 50) + 
  facet_wrap(~ stock)
```

From Figure 8 we can make some assessment on the validity of our model setup. All stocks seem to follow a unimodal distribution with mean close to 0. Approximating the returns with a multivariate distribution seems justified. Now we turn to the model setup in \texttt{Rstan}. One thing to note is that we need to restrict the output of the parameters $\alpha$ and $\beta$ to be non-negative. This is since we in the inverse-Wishart distribution need a positive definite matrix as the scale parameter (the $\nu_0 \textbf{P}_0$ in the above). Other than that there is nothing we need to keep in mind and can just define the model in \texttt{Rstan} following the model setup defined previously.

We can take a closer look at the way we define the covariance matrix $\boldsymbol{\Sigma}$. The way we define $\textbf{P}_0$ makes us inclined to think that we want no correlation between the different sectors. However, have in mind that we do not know the properties of the output for the inverse-Wishart distribution with our parameterization. But looking at a table of distributions, see [@carlin2008bayesian, Appendix A], for $V \sim$ Inv-Wishart$(\boldsymbol{\Omega}, \nu)$ where $\boldsymbol{\Omega}$ is a $k \times k$ symmetric and positive definite matrix, we have that the expected value of matrix element $V_{ij}$ is 
$$
\E(V_{ij}) = \frac{1}{\nu - k - 1} \Omega_{ij}^{-1}.
$$

Thus the covariance elements in $\boldsymbol{\Sigma}$ which corresponds to the $\boldsymbol{0}$ block in $\textbf{P}_0$ will have expected value equal to 0. So in some way we get uncorrelated sectors, but we do not know the variances of the elements in $\boldsymbol{\Sigma}$ but hopefully it is quite small for the $\boldsymbol{0}$ block. If the variance for each covariance element is not small then we may get some correlation between sectors which may impact our results. It may be valid to assume that stocks from different sectors will be uncorrelated. Also observe that the covariance between stocks in the same sector have positive expected value. This is also valid to assume since in reality stocks in the same sector often get affected by similar factors. However, for each sector block in the covariance matrix we also multiply by a factor of $\alpha$ in the IT sector and a factor of $\beta$ in the Energy sector. Both of these are standard normally distributed but only take positive values. The expected values can then change quite much and that is probably why we do this to make the covariance matrix have some bigger differences in the values of each block.    

Now let us take a look on the mean vector in our model, namely 
$$
\boldsymbol{\mu}|\mu_0, \sigma^2 \sim N(\mu_0 \boldsymbol{1}, \sigma^2 \boldsymbol{I}).
$$
We can see that each element in the vector has mean $\mu_0 \sim N(0,1)$ and variance $\sigma^2 \sim \text{Inv-Gamma}(1, 1)$. Each element in the mean vector is also uncorrelated so that we assume that the expected value of the returns of each stock is independent of the other stocks. This is fine since the correlation between stocks in same sectors are already accounted for as we have mentioned. Also to think that the expected value of the returns are independent is valid. This is because say that a company performs well and get a nice returns and thus large expected returns. If another company in the same sector is not well managed and then performs poor, this should not really affect the expected returns of other companies in the same sector (probably not much however). The parameter $\mu_0$ is standard normal and makes each company having the same expected value in the distribution of $\boldsymbol{\mu}$. The parameter $\sigma^2$ is inverse gamma distributed meaning that it can only take positive values.    

With our model defined and set up in \texttt{Rstan} we can start looking at how well it performs. First we will conduct posterior predictive checks. We will get a sample from the posterior predictive distribution and derive the summary statistics for it. 

```{r}
# Task 6 b

P_A <- matrix(c(1, 0.5, 0.5, 0, 0, 0, 
                    0.5, 1, 0.5, 0, 0, 0, 
                    0.5, 0.5, 1, 0, 0, 0,
                    rep(0, 18)
                  ),
                  nrow = 6) 

P_B <- matrix(c(0, 0, 0, 0, 0, 0, 
                   0, 0, 0, 0, 0, 0,
                   0, 0, 0, 0, 0, 0,
                   0, 0, 0, 1, 0.5, 0.5, 
                   0, 0, 0, 0.5, 1, 0.5,
                   0, 0, 0, 0.5, 0.5, 1
                  ),
                  nrow = 6)

data_stock <- data_returns %>% select(!Date) %>% as.matrix()

model_data <- list(
  "N" = length(data_returns$Date),
  "K" = 6,
  "Y" = data_stock,
  "vec_1" = rep(1, 6),
  "identity_mat" = diag(1, 6), 
  "P_alpha" = P_A, 
  "P_beta" = P_B
  )

model_fit <- stan(
  "stan_model_6.stan",
  data = model_data,
  chains = 4, 
  iter = 30000, 
  warmup = 20000,
  seed = 990108,
  cores = 4
  )
```


```{r}
rstan::extract(model_fit, pars = "y_rep") %>% 
  as.data.frame() %>% 
  rename(
    "Energy.Stock1" = "y_rep.1",
    "Energy.Stock2" = "y_rep.2",
    "Energy.Stock3" = "y_rep.3",
    "IT.Stock1" = "y_rep.4",
    "IT.Stock2" = "y_rep.5",
    "IT.Stock3" = "y_rep.6"
  ) %>% 
  pivot_longer(everything(), names_to = "stock", values_to = "returns") %>% 
  group_by(stock) %>% 
  summarise(mean = mean(returns), variance = var(returns), median = median(returns)) %>% 
  kable(
    align = "c", 
    caption = "Showing the mean, variance and median based on a sample from the 
    posterior predictive distribution.")
```

As we can see the summary statistics in Table 2 differ from the ones we got in Table 1. We have larger mean and variances in absolute value, but also the median for all of the stocks are quite different. Some stocks have reversed sign of the means. So our model does not seem to be able to predict the returns of the stocks that well. This could be a consequence of our model specification where we may not be able to capture important properties of the data. Let us now look at the convergence diagnostics of our model. First we can look at the parameters $\alpha$, $\beta$, $\sigma^2$ and $\mu_0$.

```{r, fig.cap="Traceplots for $\\alpha$, $\\beta$, $\\sigma^2$ and $\\mu_0$ with 4 chains each. A warmup of size 20000 was removed from the total of 30000 iterations."}
traceplot(model_fit, pars =c("alpha", "beta", "sigma_2", "mu_0")) + 
  xlab("iteration")
```

In Figure 9 we can see the traceplots of the parameters $\alpha$, $\beta$, $\sigma^2$ and $\mu_0$. We note that all parameters except $\mu_0$ only take positive values which is exactly what we want. Visually, convergence seems to be reached but we should also combine this with some quantitative method. The Gelman-Rubin statistic is calculated in \texttt{Rstan} and is denoted Rhat. A value of close to 1 is desired to imply convergence. For these four parameters the Rhat value is very close to 1. Now let us take a look at the more interesting parameters. 

```{r, fig.cap="Traceplots for the elements of the mean vector $\\boldsymbol{\\mu}$ with 4 chains each. A warmup of size 20000 was removed from the total of 30000 iterations."}
traceplot(model_fit, pars = "mu") + 
  xlab("iteration")
```

In Figure 10 we can see the traceplots of the mean vector. Visually it seems that we have reached some sort of convergence but we will combine this with The Gelman-Rubin statistic.   

```{r}
summary(model_fit)$summary[,"Rhat"] %>% 
  as.data.frame() %>% 
  rownames_to_column("parameter") %>% 
  rename("Rhat" = ".") %>% 
  filter(grepl("mu", parameter) & parameter != "mu_0") %>%
  pivot_wider(names_from = "parameter", values_from = "Rhat") %>% 
  kable(
    align = "c",
    col.names = c("$\\mu_1$", "$\\mu_2$", "$\\mu_3$", "$\\mu_4$", "$\\mu_5$", "$\\mu_6$"),
    caption = "Rhat values (Gelman-Rubin statistic) for the elements in the mean vector $\\boldsymbol{\\mu}$.")

#summary(model_fit)$summary[,"Rhat"] #Rhat for all parameters, are close to 1 
```

In Table 3 we can see the Rhat values of the mean vector and all of them are close to 1 which then imply that convergence of the chains is reached. We will not look at all the traceplots for the parameters since it will become quite many of them. The Rhat value is easy to check for every parameter and when doing this we find that Rhat is close to 1 for every parameter in our model. So we seem to get convergence according to the Gelman-Rubin statistic for all the parameters in the model. we can also look at the autocorrelation plots. Starting with the mean vector $\boldsymbol{\mu}$. 

```{r, fig.cap="Autocorrealtion plot of the elements in the mean vector $\\boldsymbol{\\mu}$. The autocorrelation is averaged over the 4 chains."}
stan_ac(model_fit, pars = "mu")
```

We can see In Figure 11 the autocorrelation plots of the elements in the mean vector $\boldsymbol{\mu}$. Note that the autocorrelation drops rather quickly as the time lag increases. This is desired since MCMC creates a autocorrelated sample and we want the correlation to be as small as possibly. Acceptable range of autocorrelation is as always difficult to say and varies from time to time, but here we at least seem to get it quite low. We can also do the same analysis for the variance of the stocks by looking at the diagonal elements in the covariance matrix. The Rhat value for these parameters are also close to 1 as mentioned before. 

```{r, fig.cap="Traceplots for the variance of the returns, i.e. the diagonal elements in the covariance matrix $\\boldsymbol{\\Sigma}$."}
traceplot(
  model_fit, 
  pars = c(
    "cov_mat[1,1]", 
    "cov_mat[2,2]", 
    "cov_mat[3,3]",
    "cov_mat[4,4]", 
    "cov_mat[5,5]",
    "cov_mat[6,6]"
  )
) + 
  xlab("iteration")
```

The traceplots of the variances can be seen in Figure 12. First thing to note is that they only take positive values as a variance should. Visually we can argue for convergence. The traceplots all seem to move around 1 which is supported by our calculations of the sample variance in Table 2. We can also see in Figure 13 the autocorrelation plots of the variances and may note that the autocorrelation is more present than in the case of the mean. It stays larger for longer and does not drop as rapidly. So the sample for the variances are more autocorrelated than the mean but the results may still make our overall analysis acceptable.  

```{r, fig.cap="Autocorrelation plots for the variance elements in the covariance matrix $\\boldsymbol{\\Sigma}$, i.e. the diagonal elements."}
stan_ac(
  model_fit, 
  pars = c(
    "cov_mat[1,1]", 
    "cov_mat[2,2]", 
    "cov_mat[3,3]",
    "cov_mat[4,4]", 
    "cov_mat[5,5]",
    "cov_mat[6,6]"
  )
) 
  
  
```

Now we will take some look at the non-diagonal elements in the covariance matrix as well. This because as we discussed before they have expected value 0 but we do not know about their variances. We will take a look on the upper right part of the covariance matrix that corresponds to the 0 block.

```{r, fig.height=4, fig.cap="Traceplots for the upper non-diagonal elements corresponding to covariances between the stocks, i.e. the upper non-diagonal elements in the covariance matrix $\\boldsymbol{\\Sigma}$."}
traceplot(
  model_fit, 
  pars = c(
    "cov_mat[1,4]", 
    "cov_mat[1,5]", 
    "cov_mat[1,6]",
    "cov_mat[2,4]", 
    "cov_mat[2,5]",
    "cov_mat[2,6]",
    "cov_mat[3,4]", 
    "cov_mat[3,5]",
    "cov_mat[3,6]"
  )
) + 
  xlab("iteration")
```

We can see in Figure 14 that the traceplots for the covariance between sectors are moving close to 0. This is nice since then we see that the property of uncorrelated sectors seems to hold somewhat fine. Visually, convergence seem to be attained. The other non-diagonal elements of the covariance matrix that correspond to the upper elements in $\alpha\textbf{P}_A$ we get the traceplots seen in Figure 15. The traceplots only take positive values as for the diagonal elements but they seem to be smaller and have less variance compared to variances in Figure 12. This is probably expected since these elements are multiplied by a factor of 0.5 rather than 1 for the variances. So we retain the positive correlation between stocks in the same sector.   

```{r, fig.cap="Traceplots of the non-diagonal elements in the covariance matrix $\\boldsymbol{\\Sigma}$ corresponding to the non-diagonal elements in the $\\alpha\\textbf{P}_A$ block of the matrix $\\textbf{P}_0$."}
traceplot(
  model_fit, 
  pars = c(
    "cov_mat[1,2]", 
    "cov_mat[1,3]", 
    "cov_mat[2,3]"
  )
) + 
  xlab("iteration")
```

In conclusion our model does not seem to capture the relevant properties of the data set. Our model outputs higher variance and larger mean in absolute value. The mean is in most cases up to ten times bigger than the sample mean of the data set. The sign of the mean is not preserved as well. For what reason this happens is difficult to tell. Maybe the way we have defined our model makes it not able to extract and use the relevant information in the data. But when it comes to convergence diagnostics of the model it can be argued to be satisfactory. 


```{r, eval=FALSE}
# testing area
set.seed(990108)

P_0 <-  P_alpha + P_beta   

test <- LaplacesDemon::rinvwishart(100, 100 * P_0)  

rmvnorm(
  10000, 
  mean = c(-0.07, -0.06, 0.03, -0.02, -0.05, 0.03), 
  test
) %>% 
  as.data.frame() %>% 
  mutate(mean1 = mean(V1), var1 = var(V1))
  
```

# References
